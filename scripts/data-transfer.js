#!/usr/bin/env node

/**
 * æ•°æ®å¯¼å‡º/å¯¼å…¥å·¥å…·
 *
 * ä½¿ç”¨æ–¹æ³•ï¼š
 * å¯¼å‡º: node scripts/data-transfer.js export --output=backup.json [options]
 * å¯¼å…¥: node scripts/data-transfer.js import --input=backup.json [options]
 *
 * é€‰é¡¹ï¼š
 * --types: è¦å¯¼å‡º/å¯¼å…¥çš„æ•°æ®ç±»å‹ï¼ˆapikeys,accounts,admins,allï¼‰
 * --sanitize: å¯¼å‡ºæ—¶è„±æ•æ•æ„Ÿæ•°æ®
 * --force: å¯¼å…¥æ—¶å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„æ•°æ®
 * --skip-conflicts: å¯¼å…¥æ—¶è·³è¿‡å†²çªçš„æ•°æ®
 */

const fs = require('fs').promises
const redis = require('../src/models/redis')
const logger = require('../src/utils/logger')
const readline = require('readline')

// è§£æå‘½ä»¤è¡Œå‚æ•°
const args = process.argv.slice(2)
const command = args[0]
const params = {}

args.slice(1).forEach((arg) => {
  const [key, value] = arg.split('=')
  params[key.replace('--', '')] = value || true
})

// åˆ›å»º readline æ¥å£
const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
})

async function askConfirmation(question) {
  return new Promise((resolve) => {
    rl.question(`${question} (yes/no): `, (answer) => {
      resolve(answer.toLowerCase() === 'yes' || answer.toLowerCase() === 'y')
    })
  })
}

// æ•°æ®è„±æ•å‡½æ•°
function sanitizeData(data, type) {
  const sanitized = { ...data }

  switch (type) {
    case 'apikey':
      // éšè— API Key çš„å¤§éƒ¨åˆ†å†…å®¹
      if (sanitized.apiKey) {
        sanitized.apiKey = `${sanitized.apiKey.substring(0, 10)}...[REDACTED]`
      }
      break

    case 'claude_account':
    case 'gemini_account':
      // éšè— OAuth tokens
      if (sanitized.accessToken) {
        sanitized.accessToken = '[REDACTED]'
      }
      if (sanitized.refreshToken) {
        sanitized.refreshToken = '[REDACTED]'
      }
      if (sanitized.claudeAiOauth) {
        sanitized.claudeAiOauth = '[REDACTED]'
      }
      // éšè—ä»£ç†å¯†ç 
      if (sanitized.proxyPassword) {
        sanitized.proxyPassword = '[REDACTED]'
      }
      break

    case 'admin':
      // éšè—ç®¡ç†å‘˜å¯†ç 
      if (sanitized.password) {
        sanitized.password = '[REDACTED]'
      }
      break
  }

  return sanitized
}

// CSV å­—æ®µæ˜ å°„é…ç½®
const CSV_FIELD_MAPPING = {
  // åŸºæœ¬ä¿¡æ¯
  id: 'ID',
  name: 'åç§°',
  description: 'æè¿°',
  isActive: 'çŠ¶æ€',
  createdAt: 'åˆ›å»ºæ—¶é—´',
  lastUsedAt: 'æœ€åä½¿ç”¨æ—¶é—´',
  createdBy: 'åˆ›å»ºè€…',

  // API Key ä¿¡æ¯
  apiKey: 'APIå¯†é’¥',
  tokenLimit: 'ä»¤ç‰Œé™åˆ¶',

  // è¿‡æœŸè®¾ç½®
  expirationMode: 'è¿‡æœŸæ¨¡å¼',
  expiresAt: 'è¿‡æœŸæ—¶é—´',
  activationDays: 'æ¿€æ´»å¤©æ•°',
  activationUnit: 'æ¿€æ´»å•ä½',
  isActivated: 'å·²æ¿€æ´»',
  activatedAt: 'æ¿€æ´»æ—¶é—´',

  // æƒé™è®¾ç½®
  permissions: 'æœåŠ¡æƒé™',

  // é™åˆ¶è®¾ç½®
  rateLimitWindow: 'é€Ÿç‡çª—å£(åˆ†é’Ÿ)',
  rateLimitRequests: 'è¯·æ±‚æ¬¡æ•°é™åˆ¶',
  rateLimitCost: 'è´¹ç”¨é™åˆ¶(ç¾å…ƒ)',
  concurrencyLimit: 'å¹¶å‘é™åˆ¶',
  dailyCostLimit: 'æ—¥è´¹ç”¨é™åˆ¶(ç¾å…ƒ)',
  totalCostLimit: 'æ€»è´¹ç”¨é™åˆ¶(ç¾å…ƒ)',
  weeklyOpusCostLimit: 'å‘¨Opusè´¹ç”¨é™åˆ¶(ç¾å…ƒ)',

  // è´¦æˆ·ç»‘å®š
  claudeAccountId: 'Claudeä¸“å±è´¦æˆ·',
  claudeConsoleAccountId: 'Claudeæ§åˆ¶å°è´¦æˆ·',
  geminiAccountId: 'Geminiä¸“å±è´¦æˆ·',
  openaiAccountId: 'OpenAIä¸“å±è´¦æˆ·',
  azureOpenaiAccountId: 'Azure OpenAIä¸“å±è´¦æˆ·',
  bedrockAccountId: 'Bedrockä¸“å±è´¦æˆ·',

  // é™åˆ¶é…ç½®
  enableModelRestriction: 'å¯ç”¨æ¨¡å‹é™åˆ¶',
  restrictedModels: 'é™åˆ¶çš„æ¨¡å‹',
  enableClientRestriction: 'å¯ç”¨å®¢æˆ·ç«¯é™åˆ¶',
  allowedClients: 'å…è®¸çš„å®¢æˆ·ç«¯',

  // æ ‡ç­¾å’Œç”¨æˆ·
  tags: 'æ ‡ç­¾',
  userId: 'ç”¨æˆ·ID',
  userUsername: 'ç”¨æˆ·å',

  // å…¶ä»–ä¿¡æ¯
  icon: 'å›¾æ ‡'
}

// æ•°æ®æ ¼å¼åŒ–å‡½æ•°
function formatCSVValue(key, value, shouldSanitize = false) {
  if (!value || value === '' || value === 'null' || value === 'undefined') {
    return ''
  }

  switch (key) {
    case 'apiKey':
      if (shouldSanitize && value.length > 10) {
        return `${value.substring(0, 10)}...[å·²è„±æ•]`
      }
      return value

    case 'isActive':
    case 'isActivated':
    case 'enableModelRestriction':
    case 'enableClientRestriction':
      return value === 'true' ? 'æ˜¯' : 'å¦'

    case 'expirationMode':
      return value === 'activation' ? 'é¦–æ¬¡ä½¿ç”¨åæ¿€æ´»' : value === 'fixed' ? 'å›ºå®šæ—¶é—´' : value

    case 'activationUnit':
      return value === 'hours' ? 'å°æ—¶' : value === 'days' ? 'å¤©' : value

    case 'permissions':
      switch (value) {
        case 'all':
          return 'å…¨éƒ¨æœåŠ¡'
        case 'claude':
          return 'ä»…Claude'
        case 'gemini':
          return 'ä»…Gemini'
        case 'openai':
          return 'ä»…OpenAI'
        default:
          return value
      }

    case 'restrictedModels':
    case 'allowedClients':
    case 'tags':
      try {
        const parsed = JSON.parse(value)
        return Array.isArray(parsed) ? parsed.join('; ') : value
      } catch {
        return value
      }

    case 'createdAt':
    case 'lastUsedAt':
    case 'activatedAt':
    case 'expiresAt':
      if (value) {
        try {
          return new Date(value).toLocaleString('zh-CN', {
            year: 'numeric',
            month: '2-digit',
            day: '2-digit',
            hour: '2-digit',
            minute: '2-digit',
            second: '2-digit'
          })
        } catch {
          return value
        }
      }
      return ''

    case 'rateLimitWindow':
    case 'rateLimitRequests':
    case 'concurrencyLimit':
    case 'activationDays':
    case 'tokenLimit':
      return value === '0' || value === 0 ? 'æ— é™åˆ¶' : value

    case 'rateLimitCost':
    case 'dailyCostLimit':
    case 'totalCostLimit':
    case 'weeklyOpusCostLimit':
      return value === '0' || value === 0 ? 'æ— é™åˆ¶' : `$${value}`

    default:
      return value
  }
}

// è½¬ä¹‰ CSV å­—æ®µ
function escapeCSVField(field) {
  if (field === null || field === undefined) {
    return ''
  }

  const str = String(field)

  // å¦‚æœåŒ…å«é€—å·ã€å¼•å·æˆ–æ¢è¡Œç¬¦ï¼Œéœ€è¦ç”¨å¼•å·åŒ…å›´
  if (str.includes(',') || str.includes('"') || str.includes('\n') || str.includes('\r')) {
    // å…ˆè½¬ä¹‰å¼•å·ï¼ˆåŒå¼•å·å˜æˆä¸¤ä¸ªåŒå¼•å·ï¼‰
    const escaped = str.replace(/"/g, '""')
    return `"${escaped}"`
  }

  return str
}

// è½¬æ¢æ•°æ®ä¸º CSV æ ¼å¼
function convertToCSV(exportDataObj, shouldSanitize = false) {
  if (!exportDataObj.data.apiKeys || exportDataObj.data.apiKeys.length === 0) {
    throw new Error('CSV format only supports API Keys export. Please use --types=apikeys')
  }

  const { apiKeys } = exportDataObj.data
  const fields = Object.keys(CSV_FIELD_MAPPING)
  const headers = Object.values(CSV_FIELD_MAPPING)

  // ç”Ÿæˆæ ‡é¢˜è¡Œ
  const csvLines = [headers.map(escapeCSVField).join(',')]

  // ç”Ÿæˆæ•°æ®è¡Œ
  for (const apiKey of apiKeys) {
    const row = fields.map((field) => {
      const value = formatCSVValue(field, apiKey[field], shouldSanitize)
      return escapeCSVField(value)
    })
    csvLines.push(row.join(','))
  }

  return csvLines.join('\n')
}

// å¯¼å‡ºæ•°æ®
async function exportData() {
  try {
    const format = params.format || 'json'
    const fileExtension = format === 'csv' ? '.csv' : '.json'
    const defaultFileName = `backup-${new Date().toISOString().split('T')[0]}${fileExtension}`
    const outputFile = params.output || defaultFileName
    const types = params.types ? params.types.split(',') : ['all']
    const shouldSanitize = params.sanitize === true

    // CSV æ ¼å¼éªŒè¯
    if (format === 'csv' && !types.includes('apikeys') && !types.includes('all')) {
      logger.error('âŒ CSV format only supports API Keys export. Please use --types=apikeys')
      process.exit(1)
    }

    logger.info('ğŸ”„ Starting data export...')
    logger.info(`ğŸ“ Output file: ${outputFile}`)
    logger.info(`ğŸ“‹ Data types: ${types.join(', ')}`)
    logger.info(`ğŸ“„ Output format: ${format.toUpperCase()}`)
    logger.info(`ğŸ”’ Sanitize sensitive data: ${shouldSanitize ? 'YES' : 'NO'}`)

    // è¿æ¥ Redis
    await redis.connect()
    logger.success('âœ… Connected to Redis')

    const exportDataObj = {
      metadata: {
        version: '1.0',
        exportDate: new Date().toISOString(),
        sanitized: shouldSanitize,
        types
      },
      data: {}
    }

    // å¯¼å‡º API Keys
    if (types.includes('all') || types.includes('apikeys')) {
      logger.info('ğŸ“¤ Exporting API Keys...')
      const keys = await redis.client.keys('apikey:*')
      const apiKeys = []

      for (const key of keys) {
        if (key === 'apikey:hash_map') {
          continue
        }

        // ä½¿ç”¨ hgetall è€Œä¸æ˜¯ getï¼Œå› ä¸ºæ•°æ®å­˜å‚¨åœ¨å“ˆå¸Œè¡¨ä¸­
        const data = await redis.client.hgetall(key)

        if (data && Object.keys(data).length > 0) {
          apiKeys.push(shouldSanitize ? sanitizeData(data, 'apikey') : data)
        }
      }

      exportDataObj.data.apiKeys = apiKeys
      logger.success(`âœ… Exported ${apiKeys.length} API Keys`)
    }

    // å¯¼å‡º Claude è´¦æˆ·
    if (types.includes('all') || types.includes('accounts')) {
      logger.info('ğŸ“¤ Exporting Claude accounts...')
      // æ³¨æ„ï¼šClaude è´¦æˆ·ä½¿ç”¨ claude:account: å‰ç¼€ï¼Œä¸æ˜¯ claude_account:
      const keys = await redis.client.keys('claude:account:*')
      logger.info(`Found ${keys.length} Claude account keys in Redis`)
      const accounts = []

      for (const key of keys) {
        // ä½¿ç”¨ hgetall è€Œä¸æ˜¯ getï¼Œå› ä¸ºæ•°æ®å­˜å‚¨åœ¨å“ˆå¸Œè¡¨ä¸­
        const data = await redis.client.hgetall(key)

        if (data && Object.keys(data).length > 0) {
          // è§£æ JSON å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
          if (data.claudeAiOauth) {
            try {
              data.claudeAiOauth = JSON.parse(data.claudeAiOauth)
            } catch (e) {
              // ä¿æŒåŸæ ·
            }
          }
          accounts.push(shouldSanitize ? sanitizeData(data, 'claude_account') : data)
        }
      }

      exportDataObj.data.claudeAccounts = accounts
      logger.success(`âœ… Exported ${accounts.length} Claude accounts`)

      // å¯¼å‡º Gemini è´¦æˆ·
      logger.info('ğŸ“¤ Exporting Gemini accounts...')
      const geminiKeys = await redis.client.keys('gemini_account:*')
      logger.info(`Found ${geminiKeys.length} Gemini account keys in Redis`)
      const geminiAccounts = []

      for (const key of geminiKeys) {
        // ä½¿ç”¨ hgetall è€Œä¸æ˜¯ getï¼Œå› ä¸ºæ•°æ®å­˜å‚¨åœ¨å“ˆå¸Œè¡¨ä¸­
        const data = await redis.client.hgetall(key)

        if (data && Object.keys(data).length > 0) {
          geminiAccounts.push(shouldSanitize ? sanitizeData(data, 'gemini_account') : data)
        }
      }

      exportDataObj.data.geminiAccounts = geminiAccounts
      logger.success(`âœ… Exported ${geminiAccounts.length} Gemini accounts`)
    }

    // å¯¼å‡ºç®¡ç†å‘˜
    if (types.includes('all') || types.includes('admins')) {
      logger.info('ğŸ“¤ Exporting admins...')
      const keys = await redis.client.keys('admin:*')
      const admins = []

      for (const key of keys) {
        if (key.includes('admin_username:')) {
          continue
        }

        // ä½¿ç”¨ hgetall è€Œä¸æ˜¯ getï¼Œå› ä¸ºæ•°æ®å­˜å‚¨åœ¨å“ˆå¸Œè¡¨ä¸­
        const data = await redis.client.hgetall(key)

        if (data && Object.keys(data).length > 0) {
          admins.push(shouldSanitize ? sanitizeData(data, 'admin') : data)
        }
      }

      exportDataObj.data.admins = admins
      logger.success(`âœ… Exported ${admins.length} admins`)
    }

    // æ ¹æ®æ ¼å¼å†™å…¥æ–‡ä»¶
    let fileContent
    if (format === 'csv') {
      fileContent = convertToCSV(exportDataObj, shouldSanitize)
      // æ·»åŠ  UTF-8 BOM ä»¥ä¾¿ Excel æ­£ç¡®è¯†åˆ«ä¸­æ–‡
      fileContent = `\ufeff${fileContent}`
      await fs.writeFile(outputFile, fileContent, 'utf8')
    } else {
      await fs.writeFile(outputFile, JSON.stringify(exportDataObj, null, 2))
    }

    // æ˜¾ç¤ºå¯¼å‡ºæ‘˜è¦
    console.log(`\n${'='.repeat(60)}`)
    console.log('âœ… Export Complete!')
    console.log('='.repeat(60))
    console.log(`Output file: ${outputFile}`)
    console.log(`File size: ${(await fs.stat(outputFile)).size} bytes`)

    if (exportDataObj.data.apiKeys) {
      console.log(`API Keys: ${exportDataObj.data.apiKeys.length}`)
    }
    if (exportDataObj.data.claudeAccounts) {
      console.log(`Claude Accounts: ${exportDataObj.data.claudeAccounts.length}`)
    }
    if (exportDataObj.data.geminiAccounts) {
      console.log(`Gemini Accounts: ${exportDataObj.data.geminiAccounts.length}`)
    }
    if (exportDataObj.data.admins) {
      console.log(`Admins: ${exportDataObj.data.admins.length}`)
    }
    console.log('='.repeat(60))

    if (shouldSanitize) {
      logger.warn('âš ï¸  Sensitive data has been sanitized in this export.')
    }
  } catch (error) {
    logger.error('ğŸ’¥ Export failed:', error)
    process.exit(1)
  } finally {
    await redis.disconnect()
    rl.close()
  }
}

// å¯¼å…¥æ•°æ®
async function importData() {
  try {
    const inputFile = params.input
    if (!inputFile) {
      logger.error('âŒ Please specify input file with --input=filename.json')
      process.exit(1)
    }

    const forceOverwrite = params.force === true
    const skipConflicts = params['skip-conflicts'] === true

    logger.info('ğŸ”„ Starting data import...')
    logger.info(`ğŸ“ Input file: ${inputFile}`)
    logger.info(
      `âš¡ Mode: ${forceOverwrite ? 'FORCE OVERWRITE' : skipConflicts ? 'SKIP CONFLICTS' : 'ASK ON CONFLICT'}`
    )

    // è¯»å–æ–‡ä»¶
    const fileContent = await fs.readFile(inputFile, 'utf8')
    const importDataObj = JSON.parse(fileContent)

    // éªŒè¯æ–‡ä»¶æ ¼å¼
    if (!importDataObj.metadata || !importDataObj.data) {
      logger.error('âŒ Invalid backup file format')
      process.exit(1)
    }

    logger.info(`ğŸ“… Backup date: ${importDataObj.metadata.exportDate}`)
    logger.info(`ğŸ”’ Sanitized: ${importDataObj.metadata.sanitized ? 'YES' : 'NO'}`)

    if (importDataObj.metadata.sanitized) {
      logger.warn('âš ï¸  This backup contains sanitized data. Sensitive fields will be missing!')
      const proceed = await askConfirmation('Continue with sanitized data?')
      if (!proceed) {
        logger.info('âŒ Import cancelled')
        return
      }
    }

    // æ˜¾ç¤ºå¯¼å…¥æ‘˜è¦
    console.log(`\n${'='.repeat(60)}`)
    console.log('ğŸ“‹ Import Summary:')
    console.log('='.repeat(60))
    if (importDataObj.data.apiKeys) {
      console.log(`API Keys to import: ${importDataObj.data.apiKeys.length}`)
    }
    if (importDataObj.data.claudeAccounts) {
      console.log(`Claude Accounts to import: ${importDataObj.data.claudeAccounts.length}`)
    }
    if (importDataObj.data.geminiAccounts) {
      console.log(`Gemini Accounts to import: ${importDataObj.data.geminiAccounts.length}`)
    }
    if (importDataObj.data.admins) {
      console.log(`Admins to import: ${importDataObj.data.admins.length}`)
    }
    console.log(`${'='.repeat(60)}\n`)

    // ç¡®è®¤å¯¼å…¥
    const confirmed = await askConfirmation('âš ï¸  Proceed with import?')
    if (!confirmed) {
      logger.info('âŒ Import cancelled')
      return
    }

    // è¿æ¥ Redis
    await redis.connect()
    logger.success('âœ… Connected to Redis')

    const stats = {
      imported: 0,
      skipped: 0,
      errors: 0
    }

    // å¯¼å…¥ API Keys
    if (importDataObj.data.apiKeys) {
      logger.info('\nğŸ“¥ Importing API Keys...')
      for (const apiKey of importDataObj.data.apiKeys) {
        try {
          const exists = await redis.client.exists(`apikey:${apiKey.id}`)

          if (exists && !forceOverwrite) {
            if (skipConflicts) {
              logger.warn(`â­ï¸  Skipped existing API Key: ${apiKey.name} (${apiKey.id})`)
              stats.skipped++
              continue
            } else {
              const overwrite = await askConfirmation(
                `API Key "${apiKey.name}" (${apiKey.id}) exists. Overwrite?`
              )
              if (!overwrite) {
                stats.skipped++
                continue
              }
            }
          }

          // ä½¿ç”¨ hset å­˜å‚¨åˆ°å“ˆå¸Œè¡¨
          const pipeline = redis.client.pipeline()
          for (const [field, value] of Object.entries(apiKey)) {
            pipeline.hset(`apikey:${apiKey.id}`, field, value)
          }
          await pipeline.exec()

          // æ›´æ–°å“ˆå¸Œæ˜ å°„
          if (apiKey.apiKey && !importDataObj.metadata.sanitized) {
            await redis.client.hset('apikey:hash_map', apiKey.apiKey, apiKey.id)
          }

          logger.success(`âœ… Imported API Key: ${apiKey.name} (${apiKey.id})`)
          stats.imported++
        } catch (error) {
          logger.error(`âŒ Failed to import API Key ${apiKey.id}:`, error.message)
          stats.errors++
        }
      }
    }

    // å¯¼å…¥ Claude è´¦æˆ·
    if (importDataObj.data.claudeAccounts) {
      logger.info('\nğŸ“¥ Importing Claude accounts...')
      for (const account of importDataObj.data.claudeAccounts) {
        try {
          const exists = await redis.client.exists(`claude_account:${account.id}`)

          if (exists && !forceOverwrite) {
            if (skipConflicts) {
              logger.warn(`â­ï¸  Skipped existing Claude account: ${account.name} (${account.id})`)
              stats.skipped++
              continue
            } else {
              const overwrite = await askConfirmation(
                `Claude account "${account.name}" (${account.id}) exists. Overwrite?`
              )
              if (!overwrite) {
                stats.skipped++
                continue
              }
            }
          }

          // ä½¿ç”¨ hset å­˜å‚¨åˆ°å“ˆå¸Œè¡¨
          const pipeline = redis.client.pipeline()
          for (const [field, value] of Object.entries(account)) {
            // å¦‚æœæ˜¯å¯¹è±¡ï¼Œéœ€è¦åºåˆ—åŒ–
            if (field === 'claudeAiOauth' && typeof value === 'object') {
              pipeline.hset(`claude_account:${account.id}`, field, JSON.stringify(value))
            } else {
              pipeline.hset(`claude_account:${account.id}`, field, value)
            }
          }
          await pipeline.exec()
          logger.success(`âœ… Imported Claude account: ${account.name} (${account.id})`)
          stats.imported++
        } catch (error) {
          logger.error(`âŒ Failed to import Claude account ${account.id}:`, error.message)
          stats.errors++
        }
      }
    }

    // å¯¼å…¥ Gemini è´¦æˆ·
    if (importDataObj.data.geminiAccounts) {
      logger.info('\nğŸ“¥ Importing Gemini accounts...')
      for (const account of importDataObj.data.geminiAccounts) {
        try {
          const exists = await redis.client.exists(`gemini_account:${account.id}`)

          if (exists && !forceOverwrite) {
            if (skipConflicts) {
              logger.warn(`â­ï¸  Skipped existing Gemini account: ${account.name} (${account.id})`)
              stats.skipped++
              continue
            } else {
              const overwrite = await askConfirmation(
                `Gemini account "${account.name}" (${account.id}) exists. Overwrite?`
              )
              if (!overwrite) {
                stats.skipped++
                continue
              }
            }
          }

          // ä½¿ç”¨ hset å­˜å‚¨åˆ°å“ˆå¸Œè¡¨
          const pipeline = redis.client.pipeline()
          for (const [field, value] of Object.entries(account)) {
            pipeline.hset(`gemini_account:${account.id}`, field, value)
          }
          await pipeline.exec()
          logger.success(`âœ… Imported Gemini account: ${account.name} (${account.id})`)
          stats.imported++
        } catch (error) {
          logger.error(`âŒ Failed to import Gemini account ${account.id}:`, error.message)
          stats.errors++
        }
      }
    }

    // æ˜¾ç¤ºå¯¼å…¥ç»“æœ
    console.log(`\n${'='.repeat(60)}`)
    console.log('âœ… Import Complete!')
    console.log('='.repeat(60))
    console.log(`Successfully imported: ${stats.imported}`)
    console.log(`Skipped: ${stats.skipped}`)
    console.log(`Errors: ${stats.errors}`)
    console.log('='.repeat(60))
  } catch (error) {
    logger.error('ğŸ’¥ Import failed:', error)
    process.exit(1)
  } finally {
    await redis.disconnect()
    rl.close()
  }
}

// æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
function showHelp() {
  console.log(`
Data Transfer Tool for Claude Relay Service

This tool allows you to export and import data between environments.

Usage:
  node scripts/data-transfer.js <command> [options]

Commands:
  export    Export data from Redis to a JSON file
  import    Import data from a JSON file to Redis

Export Options:
  --output=FILE        Output filename (default: backup-YYYY-MM-DD.json/.csv)
  --types=TYPE,...     Data types to export: apikeys,accounts,admins,all (default: all)
  --format=FORMAT      Output format: json,csv (default: json)
  --sanitize           Remove sensitive data from export

Import Options:
  --input=FILE         Input filename (required)
  --force              Overwrite existing data without asking
  --skip-conflicts     Skip conflicting data without asking

Examples:
  # Export all data
  node scripts/data-transfer.js export

  # Export only API keys with sanitized data
  node scripts/data-transfer.js export --types=apikeys --sanitize

  # Import data, skip conflicts
  node scripts/data-transfer.js import --input=backup.json --skip-conflicts

  # Export specific data types
  node scripts/data-transfer.js export --types=apikeys,accounts --output=prod-data.json
  
  # Export API keys to CSV format
  node scripts/data-transfer.js export --types=apikeys --format=csv --sanitize
  
  # Export to CSV with custom filename
  node scripts/data-transfer.js export --types=apikeys --format=csv --output=api-keys.csv
`)
}

// ä¸»å‡½æ•°
async function main() {
  if (!command || command === '--help' || command === 'help') {
    showHelp()
    process.exit(0)
  }

  switch (command) {
    case 'export':
      await exportData()
      break

    case 'import':
      await importData()
      break

    default:
      logger.error(`âŒ Unknown command: ${command}`)
      showHelp()
      process.exit(1)
  }
}

// è¿è¡Œ
main().catch((error) => {
  logger.error('ğŸ’¥ Unexpected error:', error)
  process.exit(1)
})
