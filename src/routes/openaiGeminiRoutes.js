const express = require('express')
const router = express.Router()
const logger = require('../utils/logger')
const { authenticateApiKey } = require('../middleware/auth')
const geminiAccountService = require('../services/geminiAccountService')
const unifiedGeminiScheduler = require('../services/unifiedGeminiScheduler')
const { getAvailableModels } = require('../services/geminiRelayService')
const crypto = require('crypto')

// ç”Ÿæˆä¼šè¯å“ˆå¸Œ
function generateSessionHash(req) {
  const sessionData = [
    req.headers['user-agent'],
    req.ip,
    req.headers['authorization']?.substring(0, 20)
  ]
    .filter(Boolean)
    .join(':')

  return crypto.createHash('sha256').update(sessionData).digest('hex')
}

// æ£€æŸ¥ API Key æƒé™
function checkPermissions(apiKeyData, requiredPermission = 'gemini') {
  const permissions = apiKeyData.permissions || 'all'
  return permissions === 'all' || permissions === requiredPermission
}

// è½¬æ¢ OpenAI æ¶ˆæ¯æ ¼å¼åˆ° Gemini æ ¼å¼
function convertMessagesToGemini(messages) {
  const contents = []
  let systemInstruction = ''

  // è¾…åŠ©å‡½æ•°ï¼šæå–æ–‡æœ¬å†…å®¹
  function extractTextContent(content) {
    // å¤„ç† null æˆ– undefined
    if (content === null || content === undefined) {
      return ''
    }

    // å¤„ç†å­—ç¬¦ä¸²
    if (typeof content === 'string') {
      return content
    }

    // å¤„ç†æ•°ç»„æ ¼å¼çš„å†…å®¹
    if (Array.isArray(content)) {
      return content
        .map((item) => {
          if (item === null || item === undefined) {
            return ''
          }
          if (typeof item === 'string') {
            return item
          }
          if (typeof item === 'object') {
            // å¤„ç† {type: 'text', text: '...'} æ ¼å¼
            if (item.type === 'text' && item.text) {
              return item.text
            }
            // å¤„ç† {text: '...'} æ ¼å¼
            if (item.text) {
              return item.text
            }
            // å¤„ç†åµŒå¥—çš„å¯¹è±¡æˆ–æ•°ç»„
            if (item.content) {
              return extractTextContent(item.content)
            }
          }
          return ''
        })
        .join('')
    }

    // å¤„ç†å¯¹è±¡æ ¼å¼çš„å†…å®¹
    if (typeof content === 'object') {
      // å¤„ç† {text: '...'} æ ¼å¼
      if (content.text) {
        return content.text
      }
      // å¤„ç† {content: '...'} æ ¼å¼
      if (content.content) {
        return extractTextContent(content.content)
      }
      // å¤„ç† {parts: [{text: '...'}]} æ ¼å¼
      if (content.parts && Array.isArray(content.parts)) {
        return content.parts
          .map((part) => {
            if (part && part.text) {
              return part.text
            }
            return ''
          })
          .join('')
      }
    }

    // æœ€åçš„åå¤‡é€‰é¡¹ï¼šåªæœ‰åœ¨å†…å®¹ç¡®å®ä¸ä¸ºç©ºä¸”æœ‰æ„ä¹‰æ—¶æ‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    if (
      content !== undefined &&
      content !== null &&
      content !== '' &&
      typeof content !== 'object'
    ) {
      return String(content)
    }

    return ''
  }

  for (const message of messages) {
    const textContent = extractTextContent(message.content)

    if (message.role === 'system') {
      systemInstruction += (systemInstruction ? '\n\n' : '') + textContent
    } else if (message.role === 'user') {
      contents.push({
        role: 'user',
        parts: [{ text: textContent }]
      })
    } else if (message.role === 'assistant') {
      contents.push({
        role: 'model',
        parts: [{ text: textContent }]
      })
    }
  }

  return { contents, systemInstruction }
}

// è½¬æ¢ Gemini å“åº”åˆ° OpenAI æ ¼å¼
function convertGeminiResponseToOpenAI(geminiResponse, model, stream = false) {
  if (stream) {
    // å¤„ç†æµå¼å“åº” - åŸæ ·è¿”å› SSE æ•°æ®
    return geminiResponse
  } else {
    // éæµå¼å“åº”è½¬æ¢
    // å¤„ç†åµŒå¥—çš„ response ç»“æ„
    const actualResponse = geminiResponse.response || geminiResponse

    if (actualResponse.candidates && actualResponse.candidates.length > 0) {
      const candidate = actualResponse.candidates[0]
      const content = candidate.content?.parts?.[0]?.text || ''
      const finishReason = candidate.finishReason?.toLowerCase() || 'stop'

      // è®¡ç®— token ä½¿ç”¨é‡
      const usage = actualResponse.usageMetadata || {
        promptTokenCount: 0,
        candidatesTokenCount: 0,
        totalTokenCount: 0
      }

      return {
        id: `chatcmpl-${Date.now()}`,
        object: 'chat.completion',
        created: Math.floor(Date.now() / 1000),
        model,
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content
            },
            finish_reason: finishReason
          }
        ],
        usage: {
          prompt_tokens: usage.promptTokenCount,
          completion_tokens: usage.candidatesTokenCount,
          total_tokens: usage.totalTokenCount
        }
      }
    } else {
      throw new Error('No response from Gemini')
    }
  }
}

// OpenAI å…¼å®¹çš„èŠå¤©å®Œæˆç«¯ç‚¹
router.post('/v1/chat/completions', authenticateApiKey, async (req, res) => {
  const startTime = Date.now()
  let abortController = null
  let account = null // Declare account outside try block for error handling
  let accountSelection = null // Declare accountSelection for error handling
  let sessionHash = null // Declare sessionHash for error handling

  try {
    const apiKeyData = req.apiKey

    // æ£€æŸ¥æƒé™
    if (!checkPermissions(apiKeyData, 'gemini')) {
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access Gemini',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }
    // å¤„ç†è¯·æ±‚ä½“ç»“æ„ - æ”¯æŒå¤šç§æ ¼å¼
    let requestBody = req.body

    // å¦‚æœè¯·æ±‚ä½“è¢«åŒ…è£…åœ¨ body å­—æ®µä¸­ï¼Œè§£åŒ…å®ƒ
    if (req.body.body && typeof req.body.body === 'object') {
      requestBody = req.body.body
    }

    // ä» URL è·¯å¾„ä¸­æå–æ¨¡å‹ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    let urlModel = null
    const urlPath = req.body?.config?.url || req.originalUrl || req.url
    const modelMatch = urlPath.match(/\/([^/]+):(?:stream)?[Gg]enerateContent/)
    if (modelMatch) {
      urlModel = modelMatch[1]
      logger.debug(`Extracted model from URL: ${urlModel}`)
    }

    // æå–è¯·æ±‚å‚æ•°
    const {
      messages: requestMessages,
      contents: requestContents,
      model: bodyModel = 'gemini-2.0-flash-exp',
      temperature = 0.7,
      max_tokens = 4096,
      stream = false
    } = requestBody

    // æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«streamæ ‡è¯†
    const isStreamFromUrl = urlPath && urlPath.includes('streamGenerateContent')
    const actualStream = stream || isStreamFromUrl

    // ä¼˜å…ˆä½¿ç”¨ URL ä¸­çš„æ¨¡å‹ï¼Œå…¶æ¬¡æ˜¯è¯·æ±‚ä½“ä¸­çš„æ¨¡å‹
    const model = urlModel || bodyModel

    // æ”¯æŒä¸¤ç§æ ¼å¼: OpenAI çš„ messages æˆ– Gemini çš„ contents
    let messages = requestMessages
    if (requestContents && Array.isArray(requestContents)) {
      messages = requestContents
    }

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!messages || !Array.isArray(messages) || messages.length === 0) {
      return res.status(400).json({
        error: {
          message: 'Messages array is required',
          type: 'invalid_request_error',
          code: 'invalid_request'
        }
      })
    }

    // æ£€æŸ¥æ¨¡å‹é™åˆ¶
    if (apiKeyData.enableModelRestriction && apiKeyData.restrictedModels.length > 0) {
      if (!apiKeyData.restrictedModels.includes(model)) {
        return res.status(403).json({
          error: {
            message: `Model ${model} is not allowed for this API key`,
            type: 'invalid_request_error',
            code: 'model_not_allowed'
          }
        })
      }
    }

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼
    const { contents: geminiContents, systemInstruction } = convertMessagesToGemini(messages)

    // æ„å»º Gemini è¯·æ±‚ä½“
    const geminiRequestBody = {
      contents: geminiContents,
      generationConfig: {
        temperature,
        maxOutputTokens: max_tokens,
        candidateCount: 1
      }
    }

    if (systemInstruction) {
      geminiRequestBody.systemInstruction = { parts: [{ text: systemInstruction }] }
    }

    // ç”Ÿæˆä¼šè¯å“ˆå¸Œç”¨äºç²˜æ€§ä¼šè¯
    sessionHash = generateSessionHash(req)

    // é€‰æ‹©å¯ç”¨çš„ Gemini è´¦æˆ·
    try {
      accountSelection = await unifiedGeminiScheduler.selectAccountForApiKey(
        apiKeyData,
        sessionHash,
        model
      )
      account = await geminiAccountService.getAccount(accountSelection.accountId)
    } catch (error) {
      logger.error('Failed to select Gemini account:', error)
      account = null
    }

    if (!account) {
      return res.status(503).json({
        error: {
          message: 'No available Gemini accounts',
          type: 'service_unavailable',
          code: 'service_unavailable'
        }
      })
    }

    logger.info(`Using Gemini account: ${account.id} for API key: ${apiKeyData.id}`)

    // æ ‡è®°è´¦æˆ·è¢«ä½¿ç”¨
    await geminiAccountService.markAccountUsed(account.id)

    // è§£æè´¦æˆ·çš„ä»£ç†é…ç½®
    let proxyConfig = null
    if (account.proxy) {
      try {
        proxyConfig = typeof account.proxy === 'string' ? JSON.parse(account.proxy) : account.proxy
      } catch (e) {
        logger.warn('Failed to parse proxy configuration:', e)
      }
    }

    // åˆ›å»ºä¸­æ­¢æ§åˆ¶å™¨
    abortController = new AbortController()

    // å¤„ç†å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
    req.on('close', () => {
      if (abortController && !abortController.signal.aborted) {
        logger.info('Client disconnected, aborting Gemini request')
        abortController.abort()
      }
    })

    // è·å–OAuthå®¢æˆ·ç«¯
    const client = await geminiAccountService.getOauthClient(
      account.accessToken,
      account.refreshToken,
      proxyConfig
    )
    if (actualStream) {
      // æµå¼å“åº”
      logger.info('StreamGenerateContent request', {
        model,
        projectId: account.projectId,
        apiKeyId: apiKeyData.id
      })

      const streamResponse = await geminiAccountService.generateContentStream(
        client,
        { model, request: geminiRequestBody },
        null, // user_prompt_id
        account.projectId, // ä½¿ç”¨æœ‰æƒé™çš„é¡¹ç›®ID
        apiKeyData.id, // ä½¿ç”¨ API Key ID ä½œä¸º session ID
        abortController.signal, // ä¼ é€’ä¸­æ­¢ä¿¡å·
        proxyConfig // ä¼ é€’ä»£ç†é…ç½®
      )

      // è®¾ç½®æµå¼å“åº”å¤´
      res.setHeader('Content-Type', 'text/event-stream')
      res.setHeader('Cache-Control', 'no-cache')
      res.setHeader('Connection', 'keep-alive')
      res.setHeader('X-Accel-Buffering', 'no')

      // å¤„ç†æµå¼å“åº”ï¼Œè½¬æ¢ä¸º OpenAI æ ¼å¼
      let buffer = ''

      // å‘é€åˆå§‹çš„ç©ºæ¶ˆæ¯ï¼Œç¬¦åˆ OpenAI æµå¼æ ¼å¼
      const initialChunk = {
        id: `chatcmpl-${Date.now()}`,
        object: 'chat.completion.chunk',
        created: Math.floor(Date.now() / 1000),
        model,
        choices: [
          {
            index: 0,
            delta: { role: 'assistant' },
            finish_reason: null
          }
        ]
      }
      res.write(`data: ${JSON.stringify(initialChunk)}\n\n`)

      // ç”¨äºæ”¶é›†usageæ•°æ®
      let totalUsage = {
        promptTokenCount: 0,
        candidatesTokenCount: 0,
        totalTokenCount: 0
      }
      const usageReported = false

      streamResponse.on('data', (chunk) => {
        try {
          const chunkStr = chunk.toString()

          if (!chunkStr.trim()) {
            return
          }

          buffer += chunkStr
          const lines = buffer.split('\n')
          buffer = lines.pop() || '' // ä¿ç•™æœ€åä¸€ä¸ªä¸å®Œæ•´çš„è¡Œ

          for (const line of lines) {
            if (!line.trim()) {
              continue
            }

            // å¤„ç† SSE æ ¼å¼
            let jsonData = line
            if (line.startsWith('data: ')) {
              jsonData = line.substring(6).trim()
            }

            if (!jsonData || jsonData === '[DONE]') {
              continue
            }

            try {
              const data = JSON.parse(jsonData)

              // æ•è·usageæ•°æ®
              if (data.response?.usageMetadata) {
                totalUsage = data.response.usageMetadata
                logger.debug('ğŸ“Š Captured Gemini usage data:', totalUsage)
              }

              // è½¬æ¢ä¸º OpenAI æµå¼æ ¼å¼
              if (data.response?.candidates && data.response.candidates.length > 0) {
                const candidate = data.response.candidates[0]
                const content = candidate.content?.parts?.[0]?.text || ''
                const { finishReason } = candidate

                // åªæœ‰å½“æœ‰å†…å®¹æˆ–è€…æ˜¯ç»“æŸæ ‡è®°æ—¶æ‰å‘é€æ•°æ®
                if (content || finishReason === 'STOP') {
                  const openaiChunk = {
                    id: `chatcmpl-${Date.now()}`,
                    object: 'chat.completion.chunk',
                    created: Math.floor(Date.now() / 1000),
                    model,
                    choices: [
                      {
                        index: 0,
                        delta: content ? { content } : {},
                        finish_reason: finishReason === 'STOP' ? 'stop' : null
                      }
                    ]
                  }

                  res.write(`data: ${JSON.stringify(openaiChunk)}\n\n`)

                  // å¦‚æœç»“æŸäº†ï¼Œæ·»åŠ  usage ä¿¡æ¯å¹¶å‘é€æœ€ç»ˆçš„ [DONE]
                  if (finishReason === 'STOP') {
                    // å¦‚æœæœ‰ usage æ•°æ®ï¼Œæ·»åŠ åˆ°æœ€åä¸€ä¸ª chunk
                    if (data.response.usageMetadata) {
                      const usageChunk = {
                        id: `chatcmpl-${Date.now()}`,
                        object: 'chat.completion.chunk',
                        created: Math.floor(Date.now() / 1000),
                        model,
                        choices: [
                          {
                            index: 0,
                            delta: {},
                            finish_reason: 'stop'
                          }
                        ],
                        usage: {
                          prompt_tokens: data.response.usageMetadata.promptTokenCount || 0,
                          completion_tokens: data.response.usageMetadata.candidatesTokenCount || 0,
                          total_tokens: data.response.usageMetadata.totalTokenCount || 0
                        }
                      }
                      res.write(`data: ${JSON.stringify(usageChunk)}\n\n`)
                    }
                    res.write('data: [DONE]\n\n')
                  }
                }
              }
            } catch (e) {
              logger.debug('Error parsing JSON line:', e.message)
            }
          }
        } catch (error) {
          logger.error('Stream processing error:', error)
          if (!res.headersSent) {
            res.status(500).json({
              error: {
                message: error.message || 'Stream error',
                type: 'api_error'
              }
            })
          }
        }
      })

      streamResponse.on('end', async () => {
        logger.info('Stream completed successfully')

        // è®°å½•ä½¿ç”¨ç»Ÿè®¡
        if (!usageReported && totalUsage.totalTokenCount > 0) {
          try {
            const apiKeyService = require('../services/apiKeyService')
            await apiKeyService.recordUsage(
              apiKeyData.id,
              totalUsage.promptTokenCount || 0,
              totalUsage.candidatesTokenCount || 0,
              0, // cacheCreateTokens
              0, // cacheReadTokens
              model,
              account.id
            )
            logger.info(
              `ğŸ“Š Recorded Gemini stream usage - Input: ${totalUsage.promptTokenCount}, Output: ${totalUsage.candidatesTokenCount}, Total: ${totalUsage.totalTokenCount}`
            )
          } catch (error) {
            logger.error('Failed to record Gemini usage:', error)
          }
        }

        if (!res.headersSent) {
          res.write('data: [DONE]\n\n')
        }
        res.end()
      })

      streamResponse.on('error', (error) => {
        logger.error('Stream error:', error)
        if (!res.headersSent) {
          res.status(500).json({
            error: {
              message: error.message || 'Stream error',
              type: 'api_error'
            }
          })
        } else {
          // å¦‚æœå·²ç»å¼€å§‹å‘é€æµæ•°æ®ï¼Œå‘é€é”™è¯¯äº‹ä»¶
          res.write(`data: {"error": {"message": "${error.message || 'Stream error'}"}}\n\n`)
          res.write('data: [DONE]\n\n')
          res.end()
        }
      })
    } else {
      // éæµå¼å“åº”
      logger.info('GenerateContent request', {
        model,
        projectId: account.projectId,
        apiKeyId: apiKeyData.id
      })

      const response = await geminiAccountService.generateContent(
        client,
        { model, request: geminiRequestBody },
        null, // user_prompt_id
        account.projectId, // ä½¿ç”¨æœ‰æƒé™çš„é¡¹ç›®ID
        apiKeyData.id, // ä½¿ç”¨ API Key ID ä½œä¸º session ID
        proxyConfig // ä¼ é€’ä»£ç†é…ç½®
      )

      // è½¬æ¢ä¸º OpenAI æ ¼å¼å¹¶è¿”å›
      const openaiResponse = convertGeminiResponseToOpenAI(response, model, false)

      // è®°å½•ä½¿ç”¨ç»Ÿè®¡
      if (openaiResponse.usage) {
        try {
          const apiKeyService = require('../services/apiKeyService')
          await apiKeyService.recordUsage(
            apiKeyData.id,
            openaiResponse.usage.prompt_tokens || 0,
            openaiResponse.usage.completion_tokens || 0,
            0, // cacheCreateTokens
            0, // cacheReadTokens
            model,
            account.id
          )
          logger.info(
            `ğŸ“Š Recorded Gemini usage - Input: ${openaiResponse.usage.prompt_tokens}, Output: ${openaiResponse.usage.completion_tokens}, Total: ${openaiResponse.usage.total_tokens}`
          )
        } catch (error) {
          logger.error('Failed to record Gemini usage:', error)
        }
      }

      res.json(openaiResponse)
    }

    const duration = Date.now() - startTime
    logger.info(`OpenAI-Gemini request completed in ${duration}ms`)
  } catch (error) {
    logger.error('OpenAI-Gemini request error:', error)

    // å¤„ç†é€Ÿç‡é™åˆ¶
    if (error.status === 429) {
      if (req.apiKey && account && accountSelection) {
        await unifiedGeminiScheduler.markAccountRateLimited(account.id, 'gemini', sessionHash)
      }
    }

    // è¿”å› OpenAI æ ¼å¼çš„é”™è¯¯å“åº”
    const status = error.status || 500
    const errorResponse = {
      error: error.error || {
        message: error.message || 'Internal server error',
        type: 'server_error',
        code: 'internal_error'
      }
    }

    res.status(status).json(errorResponse)
  } finally {
    // æ¸…ç†èµ„æº
    if (abortController) {
      abortController = null
    }
  }
  return undefined
})

// OpenAI å…¼å®¹çš„æ¨¡å‹åˆ—è¡¨ç«¯ç‚¹
router.get('/v1/models', authenticateApiKey, async (req, res) => {
  try {
    const apiKeyData = req.apiKey

    // æ£€æŸ¥æƒé™
    if (!checkPermissions(apiKeyData, 'gemini')) {
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access Gemini',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }

    // é€‰æ‹©è´¦æˆ·è·å–æ¨¡å‹åˆ—è¡¨
    let account = null
    try {
      const accountSelection = await unifiedGeminiScheduler.selectAccountForApiKey(
        apiKeyData,
        null,
        null
      )
      account = await geminiAccountService.getAccount(accountSelection.accountId)
    } catch (error) {
      logger.warn('Failed to select Gemini account for models endpoint:', error)
    }

    let models = []

    if (account) {
      // è·å–å®é™…çš„æ¨¡å‹åˆ—è¡¨
      models = await getAvailableModels(account.accessToken, account.proxy)
    } else {
      // è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
      models = [
        {
          id: 'gemini-2.0-flash-exp',
          object: 'model',
          created: Math.floor(Date.now() / 1000),
          owned_by: 'google'
        }
      ]
    }

    // å¦‚æœå¯ç”¨äº†æ¨¡å‹é™åˆ¶ï¼Œè¿‡æ»¤æ¨¡å‹åˆ—è¡¨
    if (apiKeyData.enableModelRestriction && apiKeyData.restrictedModels.length > 0) {
      models = models.filter((model) => apiKeyData.restrictedModels.includes(model.id))
    }

    res.json({
      object: 'list',
      data: models
    })
  } catch (error) {
    logger.error('Failed to get OpenAI-Gemini models:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve models',
        type: 'server_error',
        code: 'internal_error'
      }
    })
  }
  return undefined
})

// OpenAI å…¼å®¹çš„æ¨¡å‹è¯¦æƒ…ç«¯ç‚¹
router.get('/v1/models/:model', authenticateApiKey, async (req, res) => {
  try {
    const apiKeyData = req.apiKey
    const modelId = req.params.model

    // æ£€æŸ¥æƒé™
    if (!checkPermissions(apiKeyData, 'gemini')) {
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access Gemini',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }

    // æ£€æŸ¥æ¨¡å‹é™åˆ¶
    if (apiKeyData.enableModelRestriction && apiKeyData.restrictedModels.length > 0) {
      if (!apiKeyData.restrictedModels.includes(modelId)) {
        return res.status(404).json({
          error: {
            message: `Model '${modelId}' not found`,
            type: 'invalid_request_error',
            code: 'model_not_found'
          }
        })
      }
    }

    // è¿”å›æ¨¡å‹ä¿¡æ¯
    res.json({
      id: modelId,
      object: 'model',
      created: Math.floor(Date.now() / 1000),
      owned_by: 'google',
      permission: [],
      root: modelId,
      parent: null
    })
  } catch (error) {
    logger.error('Failed to get model details:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve model details',
        type: 'server_error',
        code: 'internal_error'
      }
    })
  }
  return undefined
})

module.exports = router
