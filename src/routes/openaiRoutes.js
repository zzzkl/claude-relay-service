const express = require('express')
const axios = require('axios')
const router = express.Router()
const logger = require('../utils/logger')
const { authenticateApiKey } = require('../middleware/auth')
const claudeAccountService = require('../services/claudeAccountService')
const unifiedOpenAIScheduler = require('../services/unifiedOpenAIScheduler')
const openaiAccountService = require('../services/openaiAccountService')
const apiKeyService = require('../services/apiKeyService')
const crypto = require('crypto')

// ä½¿ç”¨ç»Ÿä¸€è°ƒåº¦å™¨é€‰æ‹© OpenAI è´¦æˆ·
async function getOpenAIAuthToken(apiKeyData, sessionId = null, requestedModel = null) {
  try {
    // ç”Ÿæˆä¼šè¯å“ˆå¸Œï¼ˆå¦‚æžœæœ‰ä¼šè¯IDï¼‰
    const sessionHash = sessionId
      ? crypto.createHash('sha256').update(sessionId).digest('hex')
      : null

    // ä½¿ç”¨ç»Ÿä¸€è°ƒåº¦å™¨é€‰æ‹©è´¦æˆ·
    const result = await unifiedOpenAIScheduler.selectAccountForApiKey(
      apiKeyData,
      sessionHash,
      requestedModel
    )

    if (!result || !result.accountId) {
      throw new Error('No available OpenAI account found')
    }

    // èŽ·å–è´¦æˆ·è¯¦æƒ…
    const account = await openaiAccountService.getAccount(result.accountId)
    if (!account || !account.accessToken) {
      throw new Error(`OpenAI account ${result.accountId} has no valid accessToken`)
    }

    // è§£å¯† accessToken
    const accessToken = claudeAccountService._decryptSensitiveData(account.accessToken)
    if (!accessToken) {
      throw new Error('Failed to decrypt OpenAI accessToken')
    }

    logger.info(`Selected OpenAI account: ${account.name} (${result.accountId})`)
    return {
      accessToken,
      accountId: result.accountId,
      accountName: account.name
    }
  } catch (error) {
    logger.error('Failed to get OpenAI auth token:', error)
    throw error
  }
}

router.post('/responses', authenticateApiKey, async (req, res) => {
  let upstream = null
  try {
    // ä»Žä¸­é—´ä»¶èŽ·å– API Key æ•°æ®
    const apiKeyData = req.apiKeyData || {}

    // ä»Žè¯·æ±‚å¤´æˆ–è¯·æ±‚ä½“ä¸­æå–ä¼šè¯ ID
    const sessionId =
      req.headers['session_id'] ||
      req.headers['x-session-id'] ||
      req.body?.session_id ||
      req.body?.conversation_id ||
      null

    // ä»Žè¯·æ±‚ä½“ä¸­æå–æ¨¡åž‹å’Œæµå¼æ ‡å¿—
    const requestedModel = req.body?.model || null
    const isStream = req.body?.stream !== false // é»˜è®¤ä¸ºæµå¼ï¼ˆå…¼å®¹çŽ°æœ‰è¡Œä¸ºï¼‰

    // ä½¿ç”¨è°ƒåº¦å™¨é€‰æ‹©è´¦æˆ·
    const { accessToken, accountId } = await getOpenAIAuthToken(
      apiKeyData,
      sessionId,
      requestedModel
    )
    // åŸºäºŽç™½åå•æž„é€ ä¸Šæ¸¸æ‰€éœ€çš„è¯·æ±‚å¤´ï¼Œç¡®ä¿é”®ä¸ºå°å†™ä¸”å€¼å—æŽ§
    const incoming = req.headers || {}

    const allowedKeys = ['version', 'openai-beta', 'session_id']

    const headers = {}
    for (const key of allowedKeys) {
      if (incoming[key] !== undefined) {
        headers[key] = incoming[key]
      }
    }

    // è¦†ç›–æˆ–æ–°å¢žå¿…è¦å¤´éƒ¨
    headers['authorization'] = `Bearer ${accessToken}`
    headers['chatgpt-account-id'] = accountId
    headers['host'] = 'chatgpt.com'
    headers['accept'] = isStream ? 'text/event-stream' : 'application/json'
    headers['content-type'] = 'application/json'
    req.body['store'] = false

    // æ ¹æ® stream å‚æ•°å†³å®šè¯·æ±‚ç±»åž‹
    if (isStream) {
      // æµå¼è¯·æ±‚
      upstream = await axios.post('https://chatgpt.com/backend-api/codex/responses', req.body, {
        headers,
        responseType: 'stream',
        timeout: 60000,
        validateStatus: () => true
      })
    } else {
      // éžæµå¼è¯·æ±‚
      upstream = await axios.post('https://chatgpt.com/backend-api/codex/responses', req.body, {
        headers,
        timeout: 60000,
        validateStatus: () => true
      })
    }
    res.status(upstream.status)

    if (isStream) {
      // æµå¼å“åº”å¤´
      res.setHeader('Content-Type', 'text/event-stream')
      res.setHeader('Cache-Control', 'no-cache')
      res.setHeader('Connection', 'keep-alive')
      res.setHeader('X-Accel-Buffering', 'no')
    } else {
      // éžæµå¼å“åº”å¤´
      res.setHeader('Content-Type', 'application/json')
    }

    // é€ä¼ å…³é”®è¯Šæ–­å¤´ï¼Œé¿å…ä¼ é€’ä¸å®‰å…¨æˆ–ä¸Žä¼ è¾“ç›¸å…³çš„å¤´
    const passThroughHeaderKeys = ['openai-version', 'x-request-id', 'openai-processing-ms']
    for (const key of passThroughHeaderKeys) {
      const val = upstream.headers?.[key]
      if (val !== undefined) {
        res.setHeader(key, val)
      }
    }

    if (isStream) {
      // ç«‹å³åˆ·æ–°å“åº”å¤´ï¼Œå¼€å§‹ SSE
      if (typeof res.flushHeaders === 'function') {
        res.flushHeaders()
      }
    }

    // å¤„ç†å“åº”å¹¶æ•èŽ· usage æ•°æ®å’ŒçœŸå®žçš„ model
    let buffer = ''
    let usageData = null
    let actualModel = null
    let usageReported = false

    if (!isStream) {
      // éžæµå¼å“åº”å¤„ç†
      try {
        logger.info(`ðŸ“„ Processing OpenAI non-stream response for model: ${requestedModel}`)

        // ç›´æŽ¥èŽ·å–å®Œæ•´å“åº”
        const responseData = upstream.data

        // ä»Žå“åº”ä¸­èŽ·å–å®žé™…çš„ model å’Œ usage
        actualModel = responseData.model || requestedModel || 'gpt-4'
        usageData = responseData.usage

        logger.debug(`ðŸ“Š Non-stream response - Model: ${actualModel}, Usage:`, usageData)

        // è®°å½•ä½¿ç”¨ç»Ÿè®¡
        if (usageData) {
          const inputTokens = usageData.input_tokens || usageData.prompt_tokens || 0
          const outputTokens = usageData.output_tokens || usageData.completion_tokens || 0
          const cacheCreateTokens = usageData.input_tokens_details?.cache_creation_tokens || 0
          const cacheReadTokens = usageData.input_tokens_details?.cached_tokens || 0

          await apiKeyService.recordUsage(
            apiKeyData.id,
            inputTokens,
            outputTokens,
            cacheCreateTokens,
            cacheReadTokens,
            actualModel,
            accountId
          )

          logger.info(
            `ðŸ“Š Recorded OpenAI non-stream usage - Input: ${inputTokens}, Output: ${outputTokens}, Total: ${usageData.total_tokens || inputTokens + outputTokens}, Model: ${actualModel}`
          )
        }

        // è¿”å›žå“åº”
        res.json(responseData)
        return
      } catch (error) {
        logger.error('Failed to process non-stream response:', error)
        if (!res.headersSent) {
          res.status(500).json({ error: { message: 'Failed to process response' } })
        }
        return
      }
    }

    // è§£æž SSE äº‹ä»¶ä»¥æ•èŽ· usage æ•°æ®å’Œ model
    const parseSSEForUsage = (data) => {
      const lines = data.split('\n')

      for (const line of lines) {
        if (line.startsWith('event: response.completed')) {
          // ä¸‹ä¸€è¡Œåº”è¯¥æ˜¯æ•°æ®
          continue
        }

        if (line.startsWith('data: ')) {
          try {
            const jsonStr = line.slice(6) // ç§»é™¤ 'data: ' å‰ç¼€
            const eventData = JSON.parse(jsonStr)

            // æ£€æŸ¥æ˜¯å¦æ˜¯ response.completed äº‹ä»¶
            if (eventData.type === 'response.completed' && eventData.response) {
              // ä»Žå“åº”ä¸­èŽ·å–çœŸå®žçš„ model
              if (eventData.response.model) {
                actualModel = eventData.response.model
                logger.debug(`ðŸ“Š Captured actual model: ${actualModel}`)
              }

              // èŽ·å– usage æ•°æ®
              if (eventData.response.usage) {
                usageData = eventData.response.usage
                logger.debug('ðŸ“Š Captured OpenAI usage data:', usageData)
              }
            }
          } catch (e) {
            // å¿½ç•¥è§£æžé”™è¯¯
          }
        }
      }
    }

    upstream.data.on('data', (chunk) => {
      try {
        const chunkStr = chunk.toString()

        // è½¬å‘æ•°æ®ç»™å®¢æˆ·ç«¯
        if (!res.destroyed) {
          res.write(chunk)
        }

        // åŒæ—¶è§£æžæ•°æ®ä»¥æ•èŽ· usage ä¿¡æ¯
        buffer += chunkStr

        // å¤„ç†å®Œæ•´çš„ SSE äº‹ä»¶
        if (buffer.includes('\n\n')) {
          const events = buffer.split('\n\n')
          buffer = events.pop() || '' // ä¿ç•™æœ€åŽä¸€ä¸ªå¯èƒ½ä¸å®Œæ•´çš„äº‹ä»¶

          for (const event of events) {
            if (event.trim()) {
              parseSSEForUsage(event)
            }
          }
        }
      } catch (error) {
        logger.error('Error processing OpenAI stream chunk:', error)
      }
    })

    upstream.data.on('end', async () => {
      // å¤„ç†å‰©ä½™çš„ buffer
      if (buffer.trim()) {
        parseSSEForUsage(buffer)
      }

      // è®°å½•ä½¿ç”¨ç»Ÿè®¡
      if (!usageReported && usageData) {
        try {
          const inputTokens = usageData.input_tokens || 0
          const outputTokens = usageData.output_tokens || 0
          const cacheCreateTokens = usageData.input_tokens_details?.cache_creation_tokens || 0
          const cacheReadTokens = usageData.input_tokens_details?.cached_tokens || 0

          // ä½¿ç”¨å“åº”ä¸­çš„çœŸå®ž modelï¼Œå¦‚æžœæ²¡æœ‰åˆ™ä½¿ç”¨è¯·æ±‚ä¸­çš„ modelï¼Œæœ€åŽå›žé€€åˆ°é»˜è®¤å€¼
          const modelToRecord = actualModel || requestedModel || 'gpt-4'

          await apiKeyService.recordUsage(
            apiKeyData.id,
            inputTokens,
            outputTokens,
            cacheCreateTokens,
            cacheReadTokens,
            modelToRecord,
            accountId
          )

          logger.info(
            `ðŸ“Š Recorded OpenAI usage - Input: ${inputTokens}, Output: ${outputTokens}, Total: ${usageData.total_tokens || inputTokens + outputTokens}, Model: ${modelToRecord} (actual: ${actualModel}, requested: ${requestedModel})`
          )
          usageReported = true
        } catch (error) {
          logger.error('Failed to record OpenAI usage:', error)
        }
      }

      res.end()
    })

    upstream.data.on('error', (err) => {
      logger.error('Upstream stream error:', err)
      if (!res.headersSent) {
        res.status(502).json({ error: { message: 'Upstream stream error' } })
      } else {
        res.end()
      }
    })

    // å®¢æˆ·ç«¯æ–­å¼€æ—¶æ¸…ç†ä¸Šæ¸¸æµ
    const cleanup = () => {
      try {
        upstream.data?.unpipe?.(res)
        upstream.data?.destroy?.()
      } catch (_) {
        //
      }
    }
    req.on('close', cleanup)
    req.on('aborted', cleanup)
  } catch (error) {
    logger.error('Proxy to ChatGPT codex/responses failed:', error)
    const status = error.response?.status || 500
    const message = error.response?.data || error.message || 'Internal server error'
    if (!res.headersSent) {
      res.status(status).json({ error: { message } })
    }
  }
})

// ä½¿ç”¨æƒ…å†µç»Ÿè®¡ç«¯ç‚¹
router.get('/usage', authenticateApiKey, async (req, res) => {
  try {
    const { usage } = req.apiKey

    res.json({
      object: 'usage',
      total_tokens: usage.total.tokens,
      total_requests: usage.total.requests,
      daily_tokens: usage.daily.tokens,
      daily_requests: usage.daily.requests,
      monthly_tokens: usage.monthly.tokens,
      monthly_requests: usage.monthly.requests
    })
  } catch (error) {
    logger.error('Failed to get usage stats:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve usage statistics',
        type: 'api_error'
      }
    })
  }
})

// API Key ä¿¡æ¯ç«¯ç‚¹
router.get('/key-info', authenticateApiKey, async (req, res) => {
  try {
    const keyData = req.apiKey
    res.json({
      id: keyData.id,
      name: keyData.name,
      description: keyData.description,
      permissions: keyData.permissions || 'all',
      token_limit: keyData.tokenLimit,
      tokens_used: keyData.usage.total.tokens,
      tokens_remaining:
        keyData.tokenLimit > 0
          ? Math.max(0, keyData.tokenLimit - keyData.usage.total.tokens)
          : null,
      rate_limit: {
        window: keyData.rateLimitWindow,
        requests: keyData.rateLimitRequests
      },
      usage: {
        total: keyData.usage.total,
        daily: keyData.usage.daily,
        monthly: keyData.usage.monthly
      }
    })
  } catch (error) {
    logger.error('Failed to get key info:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve API key information',
        type: 'api_error'
      }
    })
  }
})

module.exports = router
