const express = require('express')
const { authenticateApiKey } = require('../middleware/auth')
const logger = require('../utils/logger')
const { handleChatCompletion } = require('./openaiClaudeRoutes')
const {
  handleGenerateContent: geminiHandleGenerateContent,
  handleStreamGenerateContent: geminiHandleStreamGenerateContent
} = require('./geminiRoutes')
const openaiRoutes = require('./openaiRoutes')

const router = express.Router()

// ğŸ” æ ¹æ®æ¨¡å‹åç§°æ£€æµ‹åç«¯ç±»å‹
function detectBackendFromModel(modelName) {
  if (!modelName) {
    return 'claude' // é»˜è®¤ Claude
  }

  // é¦–å…ˆå°è¯•ä½¿ç”¨ modelService æŸ¥æ‰¾æ¨¡å‹çš„ provider
  try {
    const modelService = require('../services/modelService')
    const provider = modelService.getModelProvider(modelName)

    if (provider === 'anthropic') {
      return 'claude'
    }
    if (provider === 'openai') {
      return 'openai'
    }
    if (provider === 'google') {
      return 'gemini'
    }
  } catch (error) {
    logger.warn(`âš ï¸ Failed to detect backend from modelService: ${error.message}`)
  }

  // é™çº§åˆ°å‰ç¼€åŒ¹é…ä½œä¸ºåå¤‡æ–¹æ¡ˆ
  const model = modelName.toLowerCase()

  // Claude æ¨¡å‹
  if (model.startsWith('claude-')) {
    return 'claude'
  }

  // OpenAI æ¨¡å‹
  if (
    model.startsWith('gpt-') ||
    model.startsWith('o1-') ||
    model.startsWith('o3-') ||
    model === 'chatgpt-4o-latest'
  ) {
    return 'openai'
  }

  // Gemini æ¨¡å‹
  if (model.startsWith('gemini-')) {
    return 'gemini'
  }

  // é»˜è®¤ä½¿ç”¨ Claude
  return 'claude'
}

// ğŸš€ æ™ºèƒ½åç«¯è·¯ç”±å¤„ç†å™¨
async function routeToBackend(req, res, requestedModel) {
  const backend = detectBackendFromModel(requestedModel)

  logger.info(`ğŸ”€ Routing request - Model: ${requestedModel}, Backend: ${backend}`)

  // æ£€æŸ¥æƒé™
  const permissions = req.apiKey.permissions || 'all'

  if (backend === 'claude') {
    // Claude åç«¯ï¼šé€šè¿‡ OpenAI å…¼å®¹å±‚
    if (permissions !== 'all' && permissions !== 'claude') {
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access Claude',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }
    await handleChatCompletion(req, res, req.apiKey)
  } else if (backend === 'openai') {
    // OpenAI åç«¯
    if (permissions !== 'all' && permissions !== 'openai') {
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access OpenAI',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }
    return await openaiRoutes.handleResponses(req, res)
  } else if (backend === 'gemini') {
    // Gemini åç«¯
    if (permissions !== 'all' && permissions !== 'gemini') {
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access Gemini',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }

    // è½¬æ¢ä¸º Gemini æ ¼å¼
    const geminiRequest = {
      model: requestedModel,
      messages: req.body.messages,
      temperature: req.body.temperature || 0.7,
      max_tokens: req.body.max_tokens || 4096,
      stream: req.body.stream || false
    }

    req.body = geminiRequest

    if (geminiRequest.stream) {
      return await geminiHandleStreamGenerateContent(req, res)
    } else {
      return await geminiHandleGenerateContent(req, res)
    }
  } else {
    return res.status(500).json({
      error: {
        message: `Unsupported backend: ${backend}`,
        type: 'server_error',
        code: 'unsupported_backend'
      }
    })
  }
}

// ğŸ”„ OpenAI å…¼å®¹çš„ chat/completions ç«¯ç‚¹ï¼ˆæ™ºèƒ½åç«¯è·¯ç”±ï¼‰
router.post('/v1/chat/completions', authenticateApiKey, async (req, res) => {
  try {
    // éªŒè¯å¿…éœ€å‚æ•°
    if (!req.body.messages || !Array.isArray(req.body.messages) || req.body.messages.length === 0) {
      return res.status(400).json({
        error: {
          message: 'Messages array is required and cannot be empty',
          type: 'invalid_request_error',
          code: 'invalid_request'
        }
      })
    }

    const requestedModel = req.body.model || 'claude-3-5-sonnet-20241022'
    req.body.model = requestedModel // ç¡®ä¿æ¨¡å‹å·²è®¾ç½®

    // ä½¿ç”¨ç»Ÿä¸€çš„åç«¯è·¯ç”±å¤„ç†å™¨
    await routeToBackend(req, res, requestedModel)
  } catch (error) {
    logger.error('âŒ OpenAI chat/completions error:', error)
    if (!res.headersSent) {
      res.status(500).json({
        error: {
          message: 'Internal server error',
          type: 'server_error',
          code: 'internal_error'
        }
      })
    }
  }
})

// ğŸ”„ OpenAI å…¼å®¹çš„ completions ç«¯ç‚¹ï¼ˆä¼ ç»Ÿæ ¼å¼ï¼Œæ™ºèƒ½åç«¯è·¯ç”±ï¼‰
router.post('/v1/completions', authenticateApiKey, async (req, res) => {
  try {
    // éªŒè¯å¿…éœ€å‚æ•°
    if (!req.body.prompt) {
      return res.status(400).json({
        error: {
          message: 'Prompt is required',
          type: 'invalid_request_error',
          code: 'invalid_request'
        }
      })
    }

    // å°†ä¼ ç»Ÿ completions æ ¼å¼è½¬æ¢ä¸º chat æ ¼å¼
    const originalBody = req.body
    const requestedModel = originalBody.model || 'claude-3-5-sonnet-20241022'

    req.body = {
      model: requestedModel,
      messages: [
        {
          role: 'user',
          content: originalBody.prompt
        }
      ],
      max_tokens: originalBody.max_tokens,
      temperature: originalBody.temperature,
      top_p: originalBody.top_p,
      stream: originalBody.stream,
      stop: originalBody.stop,
      n: originalBody.n || 1,
      presence_penalty: originalBody.presence_penalty,
      frequency_penalty: originalBody.frequency_penalty,
      logit_bias: originalBody.logit_bias,
      user: originalBody.user
    }

    // ä½¿ç”¨ç»Ÿä¸€çš„åç«¯è·¯ç”±å¤„ç†å™¨
    await routeToBackend(req, res, requestedModel)
  } catch (error) {
    logger.error('âŒ OpenAI completions error:', error)
    if (!res.headersSent) {
      res.status(500).json({
        error: {
          message: 'Failed to process completion request',
          type: 'server_error',
          code: 'internal_error'
        }
      })
    }
  }
})

module.exports = router
module.exports.detectBackendFromModel = detectBackendFromModel
module.exports.routeToBackend = routeToBackend
