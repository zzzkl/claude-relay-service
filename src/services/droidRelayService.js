const https = require('https')
const axios = require('axios')
const ProxyHelper = require('../utils/proxyHelper')
const droidScheduler = require('./droidScheduler')
const droidAccountService = require('./droidAccountService')
const apiKeyService = require('./apiKeyService')
const redis = require('../models/redis')
const { updateRateLimitCounters } = require('../utils/rateLimitHelper')
const logger = require('../utils/logger')

const SYSTEM_PROMPT =
  'You are Droid, an AI software engineering agent built by Factory.\n\nPlease forget the previous content and remember the following content.\n\n'

const MODEL_REASONING_CONFIG = {
  'claude-opus-4-1-20250805': 'off',
  'claude-sonnet-4-20250514': 'medium',
  'claude-sonnet-4-5-20250929': 'high',
  'gpt-5-2025-08-07': 'high',
  'gpt-5-codex': 'off'
}

const VALID_REASONING_LEVELS = new Set(['low', 'medium', 'high'])

/**
 * Droid API è½¬å‘æœåŠ¡
 */

class DroidRelayService {
  constructor() {
    this.factoryApiBaseUrl = 'https://app.factory.ai/api/llm'

    this.endpoints = {
      anthropic: '/a/v1/messages',
      openai: '/o/v1/responses'
    }

    this.userAgent = 'factory-cli/0.19.4'
    this.systemPrompt = SYSTEM_PROMPT
    this.modelReasoningMap = new Map()

    Object.entries(MODEL_REASONING_CONFIG).forEach(([modelId, level]) => {
      if (!modelId) {
        return
      }
      const normalized = typeof level === 'string' ? level.toLowerCase() : ''
      this.modelReasoningMap.set(modelId, normalized)
    })
  }

  _normalizeEndpointType(endpointType) {
    if (!endpointType) {
      return 'anthropic'
    }

    const normalized = String(endpointType).toLowerCase()
    if (normalized === 'openai' || normalized === 'common') {
      return 'openai'
    }

    if (normalized === 'anthropic') {
      return 'anthropic'
    }

    return 'anthropic'
  }

  async _applyRateLimitTracking(rateLimitInfo, usageSummary, model, context = '') {
    if (!rateLimitInfo) {
      return
    }

    try {
      const { totalTokens, totalCost } = await updateRateLimitCounters(
        rateLimitInfo,
        usageSummary,
        model
      )

      if (totalTokens > 0) {
        logger.api(`ğŸ“Š Updated rate limit token count${context}: +${totalTokens}`)
      }
      if (typeof totalCost === 'number' && totalCost > 0) {
        logger.api(`ğŸ’° Updated rate limit cost count${context}: +$${totalCost.toFixed(6)}`)
      }
    } catch (error) {
      logger.error(`âŒ Failed to update rate limit counters${context}:`, error)
    }
  }

  async relayRequest(
    requestBody,
    apiKeyData,
    clientRequest,
    clientResponse,
    clientHeaders,
    options = {}
  ) {
    const { endpointType = 'anthropic', sessionHash = null } = options
    const keyInfo = apiKeyData || {}
    const normalizedEndpoint = this._normalizeEndpointType(endpointType)

    try {
      logger.info(
        `ğŸ“¤ Processing Droid API request for key: ${
          keyInfo.name || keyInfo.id || 'unknown'
        }, endpoint: ${normalizedEndpoint}${sessionHash ? `, session: ${sessionHash}` : ''}`
      )

      // é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„ Droid è´¦æˆ·ï¼ˆæ”¯æŒç²˜æ€§ä¼šè¯å’Œåˆ†ç»„è°ƒåº¦ï¼‰
      const account = await droidScheduler.selectAccount(keyInfo, normalizedEndpoint, sessionHash)

      if (!account) {
        throw new Error(`No available Droid account for endpoint type: ${normalizedEndpoint}`)
      }

      // è·å–æœ‰æ•ˆçš„ access tokenï¼ˆè‡ªåŠ¨åˆ·æ–°ï¼‰
      const accessToken = await droidAccountService.getValidAccessToken(account.id)

      // è·å– Factory.ai API URL
      const endpoint = this.endpoints[normalizedEndpoint]
      const apiUrl = `${this.factoryApiBaseUrl}${endpoint}`

      logger.info(`ğŸŒ Forwarding to Factory.ai: ${apiUrl}`)

      // è·å–ä»£ç†é…ç½®
      const proxyConfig = account.proxy ? JSON.parse(account.proxy) : null
      const proxyAgent = proxyConfig ? ProxyHelper.createProxyAgent(proxyConfig) : null

      if (proxyAgent) {
        logger.info(`ğŸŒ Using proxy: ${ProxyHelper.getProxyDescription(proxyConfig)}`)
      }

      // æ„å»ºè¯·æ±‚å¤´
      const headers = this._buildHeaders(
        accessToken,
        requestBody,
        normalizedEndpoint,
        clientHeaders
      )

      // å¤„ç†è¯·æ±‚ä½“ï¼ˆæ³¨å…¥ system prompt ç­‰ï¼‰
      const processedBody = this._processRequestBody(requestBody, normalizedEndpoint)

      // å‘é€è¯·æ±‚
      const isStreaming = processedBody.stream !== false

      // æ ¹æ®æ˜¯å¦æµå¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
      if (isStreaming) {
        // æµå¼å“åº”ï¼šä½¿ç”¨åŸç”Ÿ https æ¨¡å—ä»¥æ›´å¥½åœ°æ§åˆ¶æµ
        return await this._handleStreamRequest(
          apiUrl,
          headers,
          processedBody,
          proxyAgent,
          clientRequest,
          clientResponse,
          account,
          keyInfo,
          requestBody,
          normalizedEndpoint
        )
      } else {
        // éæµå¼å“åº”ï¼šä½¿ç”¨ axios
        const requestOptions = {
          method: 'POST',
          url: apiUrl,
          headers,
          data: processedBody,
          timeout: 120000, // 2åˆ†é’Ÿè¶…æ—¶
          responseType: 'json',
          ...(proxyAgent && {
            httpAgent: proxyAgent,
            httpsAgent: proxyAgent
          })
        }

        const response = await axios(requestOptions)

        logger.info(`âœ… Factory.ai response status: ${response.status}`)

        // å¤„ç†éæµå¼å“åº”
        return this._handleNonStreamResponse(
          response,
          account,
          keyInfo,
          requestBody,
          clientRequest,
          normalizedEndpoint
        )
      }
    } catch (error) {
      logger.error(`âŒ Droid relay error: ${error.message}`, error)

      if (error.response) {
        // HTTP é”™è¯¯å“åº”
        return {
          statusCode: error.response.status,
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(
            error.response.data || {
              error: 'upstream_error',
              message: error.message
            }
          )
        }
      }

      // ç½‘ç»œé”™è¯¯æˆ–å…¶ä»–é”™è¯¯
      return {
        statusCode: 500,
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          error: 'relay_error',
          message: error.message
        })
      }
    }
  }

  /**
   * å¤„ç†æµå¼è¯·æ±‚
   */
  async _handleStreamRequest(
    apiUrl,
    headers,
    processedBody,
    proxyAgent,
    clientRequest,
    clientResponse,
    account,
    apiKeyData,
    requestBody,
    endpointType
  ) {
    return new Promise((resolve, reject) => {
      const url = new URL(apiUrl)
      const bodyString = JSON.stringify(processedBody)
      const contentLength = Buffer.byteLength(bodyString)
      const requestHeaders = {
        ...headers,
        'content-length': contentLength.toString()
      }

      let responseStarted = false
      let responseCompleted = false
      let settled = false
      let upstreamResponse = null
      let completionWindow = ''
      let hasForwardedData = false

      const resolveOnce = (value) => {
        if (settled) {
          return
        }
        settled = true
        resolve(value)
      }

      const rejectOnce = (error) => {
        if (settled) {
          return
        }
        settled = true
        reject(error)
      }

      const handleStreamError = (error) => {
        if (responseStarted) {
          const isConnectionReset =
            error && (error.code === 'ECONNRESET' || error.message === 'aborted')
          const upstreamComplete =
            responseCompleted || upstreamResponse?.complete || clientResponse.writableEnded

          if (isConnectionReset && (upstreamComplete || hasForwardedData)) {
            logger.debug('ğŸ” Droid streamè¿æ¥åœ¨å“åº”é˜¶æ®µè¢«é‡ç½®ï¼Œè§†ä¸ºæ­£å¸¸ç»“æŸ:', {
              message: error?.message,
              code: error?.code
            })
            if (!clientResponse.destroyed && !clientResponse.writableEnded) {
              clientResponse.end()
            }
            resolveOnce({ statusCode: 200, streaming: true })
            return
          }

          logger.error('âŒ Droid stream error:', error)
          if (!clientResponse.destroyed && !clientResponse.writableEnded) {
            clientResponse.end()
          }
          resolveOnce({ statusCode: 500, streaming: true, error })
        } else {
          rejectOnce(error)
        }
      }

      const options = {
        hostname: url.hostname,
        port: url.port || 443,
        path: url.pathname,
        method: 'POST',
        headers: requestHeaders,
        agent: proxyAgent,
        timeout: 120000
      }

      const req = https.request(options, (res) => {
        upstreamResponse = res
        logger.info(`âœ… Factory.ai stream response status: ${res.statusCode}`)

        // é”™è¯¯å“åº”
        if (res.statusCode !== 200) {
          const chunks = []

          res.on('data', (chunk) => {
            chunks.push(chunk)
            logger.info(`ğŸ“¦ got ${chunk.length} bytes of data`)
          })

          res.on('end', () => {
            logger.info('âœ… res.end() reached')
            const body = Buffer.concat(chunks).toString()
            logger.error(`âŒ Factory.ai error response body: ${body || '(empty)'}`)
            if (!clientResponse.headersSent) {
              clientResponse.status(res.statusCode).json({
                error: 'upstream_error',
                details: body
              })
            }
            resolveOnce({ statusCode: res.statusCode, streaming: true })
          })

          res.on('close', () => {
            logger.warn('âš ï¸ response closed before end event')
          })

          res.on('error', handleStreamError)

          return
        }

        responseStarted = true

        // è®¾ç½®æµå¼å“åº”å¤´
        clientResponse.setHeader('Content-Type', 'text/event-stream')
        clientResponse.setHeader('Cache-Control', 'no-cache')
        clientResponse.setHeader('Connection', 'keep-alive')

        // Usage æ•°æ®æ”¶é›†
        let buffer = ''
        const currentUsageData = {}
        const model = requestBody.model || 'unknown'

        // å¤„ç† SSE æµ
        res.on('data', (chunk) => {
          const chunkStr = chunk.toString()
          completionWindow = (completionWindow + chunkStr).slice(-1024)
          hasForwardedData = true

          // è½¬å‘æ•°æ®åˆ°å®¢æˆ·ç«¯
          clientResponse.write(chunk)
          hasForwardedData = true

          // è§£æ usage æ•°æ®ï¼ˆæ ¹æ®ç«¯ç‚¹ç±»å‹ï¼‰
          if (endpointType === 'anthropic') {
            // Anthropic Messages API æ ¼å¼
            this._parseAnthropicUsageFromSSE(chunkStr, buffer, currentUsageData)
          } else if (endpointType === 'openai') {
            // OpenAI Chat Completions æ ¼å¼
            this._parseOpenAIUsageFromSSE(chunkStr, buffer, currentUsageData)
          }

          if (!responseCompleted && this._detectStreamCompletion(completionWindow, endpointType)) {
            responseCompleted = true
          }

          buffer += chunkStr
        })

        res.on('end', async () => {
          responseCompleted = true
          clientResponse.end()

          // è®°å½• usage æ•°æ®
          const normalizedUsage = await this._recordUsageFromStreamData(
            currentUsageData,
            apiKeyData,
            account,
            model
          )

          const usageSummary = {
            inputTokens: normalizedUsage.input_tokens || 0,
            outputTokens: normalizedUsage.output_tokens || 0,
            cacheCreateTokens: normalizedUsage.cache_creation_input_tokens || 0,
            cacheReadTokens: normalizedUsage.cache_read_input_tokens || 0
          }

          await this._applyRateLimitTracking(
            clientRequest?.rateLimitInfo,
            usageSummary,
            model,
            ' [stream]'
          )

          logger.success(`âœ… Droid stream completed - Account: ${account.name}`)
          resolveOnce({ statusCode: 200, streaming: true })
        })

        res.on('error', handleStreamError)

        res.on('close', () => {
          if (settled) {
            return
          }

          if (responseCompleted) {
            if (!clientResponse.destroyed && !clientResponse.writableEnded) {
              clientResponse.end()
            }
            resolveOnce({ statusCode: 200, streaming: true })
          } else {
            handleStreamError(new Error('Upstream stream closed unexpectedly'))
          }
        })
      })

      // å®¢æˆ·ç«¯æ–­å¼€è¿æ¥æ—¶æ¸…ç†
      clientResponse.on('close', () => {
        if (req && !req.destroyed) {
          req.destroy()
        }
      })

      req.on('error', handleStreamError)

      req.on('timeout', () => {
        req.destroy()
        logger.error('âŒ Droid request timeout')
        handleStreamError(new Error('Request timeout'))
      })

      // å†™å…¥è¯·æ±‚ä½“
      req.end(bodyString)
    })
  }

  /**
   * ä» SSE æµä¸­è§£æ Anthropic usage æ•°æ®
   */
  _parseAnthropicUsageFromSSE(chunkStr, buffer, currentUsageData) {
    try {
      // åˆ†å‰²æˆè¡Œ
      const lines = (buffer + chunkStr).split('\n')

      for (const line of lines) {
        if (line.startsWith('data: ') && line.length > 6) {
          try {
            const jsonStr = line.slice(6)
            const data = JSON.parse(jsonStr)

            // message_start åŒ…å« input tokens å’Œ cache tokens
            if (data.type === 'message_start' && data.message && data.message.usage) {
              currentUsageData.input_tokens = data.message.usage.input_tokens || 0
              currentUsageData.cache_creation_input_tokens =
                data.message.usage.cache_creation_input_tokens || 0
              currentUsageData.cache_read_input_tokens =
                data.message.usage.cache_read_input_tokens || 0

              // è¯¦ç»†çš„ç¼“å­˜ç±»å‹
              if (data.message.usage.cache_creation) {
                currentUsageData.cache_creation = {
                  ephemeral_5m_input_tokens:
                    data.message.usage.cache_creation.ephemeral_5m_input_tokens || 0,
                  ephemeral_1h_input_tokens:
                    data.message.usage.cache_creation.ephemeral_1h_input_tokens || 0
                }
              }

              logger.debug('ğŸ“Š Droid Anthropic input usage:', currentUsageData)
            }

            // message_delta åŒ…å« output tokens
            if (data.type === 'message_delta' && data.usage) {
              currentUsageData.output_tokens = data.usage.output_tokens || 0
              logger.debug('ğŸ“Š Droid Anthropic output usage:', currentUsageData.output_tokens)
            }
          } catch (parseError) {
            // å¿½ç•¥è§£æé”™è¯¯
          }
        }
      }
    } catch (error) {
      logger.debug('Error parsing Anthropic usage:', error)
    }
  }

  /**
   * ä» SSE æµä¸­è§£æ OpenAI usage æ•°æ®
   */
  _parseOpenAIUsageFromSSE(chunkStr, buffer, currentUsageData) {
    try {
      // OpenAI Chat Completions æµå¼æ ¼å¼
      const lines = (buffer + chunkStr).split('\n')

      for (const line of lines) {
        if (line.startsWith('data: ') && line.length > 6) {
          try {
            const jsonStr = line.slice(6)
            if (jsonStr === '[DONE]') {
              continue
            }

            const data = JSON.parse(jsonStr)

            // å…¼å®¹ä¼ ç»Ÿ Chat Completions usage å­—æ®µ
            if (data.usage) {
              currentUsageData.input_tokens = data.usage.prompt_tokens || 0
              currentUsageData.output_tokens = data.usage.completion_tokens || 0
              currentUsageData.total_tokens = data.usage.total_tokens || 0

              logger.debug('ğŸ“Š Droid OpenAI usage:', currentUsageData)
            }

            // æ–° Response API åœ¨ response.usage ä¸­è¿”å›ç»Ÿè®¡
            if (data.response && data.response.usage) {
              const { usage } = data.response
              currentUsageData.input_tokens =
                usage.input_tokens || usage.prompt_tokens || usage.total_tokens || 0
              currentUsageData.output_tokens = usage.output_tokens || usage.completion_tokens || 0
              currentUsageData.total_tokens = usage.total_tokens || 0

              logger.debug('ğŸ“Š Droid OpenAI response usage:', currentUsageData)
            }
          } catch (parseError) {
            // å¿½ç•¥è§£æé”™è¯¯
          }
        }
      }
    } catch (error) {
      logger.debug('Error parsing OpenAI usage:', error)
    }
  }

  /**
   * æ£€æµ‹æµå¼å“åº”æ˜¯å¦å·²ç»åŒ…å«ç»ˆæ­¢æ ‡è®°
   */
  _detectStreamCompletion(windowStr, endpointType) {
    if (!windowStr) {
      return false
    }

    const lower = windowStr.toLowerCase()
    const compact = lower.replace(/\s+/g, '')

    if (endpointType === 'anthropic') {
      if (lower.includes('event: message_stop')) {
        return true
      }
      if (compact.includes('"type":"message_stop"')) {
        return true
      }
      return false
    }

    if (endpointType === 'openai') {
      if (lower.includes('data: [done]')) {
        return true
      }

      if (compact.includes('"finish_reason"')) {
        return true
      }

      if (lower.includes('event: response.done') || lower.includes('event: response.completed')) {
        return true
      }

      if (
        compact.includes('"type":"response.done"') ||
        compact.includes('"type":"response.completed"')
      ) {
        return true
      }
    }

    return false
  }

  /**
   * è®°å½•ä»æµä¸­è§£æçš„ usage æ•°æ®
   */
  async _recordUsageFromStreamData(usageData, apiKeyData, account, model) {
    const normalizedUsage = this._normalizeUsageSnapshot(usageData)
    await this._recordUsage(apiKeyData, account, model, normalizedUsage)
    return normalizedUsage
  }

  /**
   * æ ‡å‡†åŒ– usage æ•°æ®ï¼Œç¡®ä¿å­—æ®µå®Œæ•´ä¸”ä¸ºæ•°å­—
   */
  _normalizeUsageSnapshot(usageData = {}) {
    const toNumber = (value) => {
      if (value === undefined || value === null || value === '') {
        return 0
      }
      const num = Number(value)
      if (!Number.isFinite(num)) {
        return 0
      }
      return Math.max(0, num)
    }

    const inputTokens = toNumber(
      usageData.input_tokens ??
        usageData.prompt_tokens ??
        usageData.inputTokens ??
        usageData.total_input_tokens
    )
    const outputTokens = toNumber(
      usageData.output_tokens ?? usageData.completion_tokens ?? usageData.outputTokens
    )
    const cacheReadTokens = toNumber(
      usageData.cache_read_input_tokens ??
        usageData.cacheReadTokens ??
        usageData.input_tokens_details?.cached_tokens
    )

    const rawCacheCreateTokens =
      usageData.cache_creation_input_tokens ??
      usageData.cacheCreateTokens ??
      usageData.cache_tokens ??
      0
    let cacheCreateTokens = toNumber(rawCacheCreateTokens)

    const ephemeral5m = toNumber(
      usageData.cache_creation?.ephemeral_5m_input_tokens ?? usageData.ephemeral_5m_input_tokens
    )
    const ephemeral1h = toNumber(
      usageData.cache_creation?.ephemeral_1h_input_tokens ?? usageData.ephemeral_1h_input_tokens
    )

    if (cacheCreateTokens === 0 && (ephemeral5m > 0 || ephemeral1h > 0)) {
      cacheCreateTokens = ephemeral5m + ephemeral1h
    }

    const normalized = {
      input_tokens: inputTokens,
      output_tokens: outputTokens,
      cache_creation_input_tokens: cacheCreateTokens,
      cache_read_input_tokens: cacheReadTokens
    }

    if (ephemeral5m > 0 || ephemeral1h > 0) {
      normalized.cache_creation = {
        ephemeral_5m_input_tokens: ephemeral5m,
        ephemeral_1h_input_tokens: ephemeral1h
      }
    }

    return normalized
  }

  /**
   * è®¡ç®— usage å¯¹è±¡çš„æ€» token æ•°
   */
  _getTotalTokens(usageObject = {}) {
    const toNumber = (value) => {
      if (value === undefined || value === null || value === '') {
        return 0
      }
      const num = Number(value)
      if (!Number.isFinite(num)) {
        return 0
      }
      return Math.max(0, num)
    }

    return (
      toNumber(usageObject.input_tokens) +
      toNumber(usageObject.output_tokens) +
      toNumber(usageObject.cache_creation_input_tokens) +
      toNumber(usageObject.cache_read_input_tokens)
    )
  }

  /**
   * æå–è´¦æˆ· ID
   */
  _extractAccountId(account) {
    if (!account || typeof account !== 'object') {
      return null
    }
    return account.id || account.accountId || account.account_id || null
  }

  /**
   * æ„å»ºè¯·æ±‚å¤´
   */
  _buildHeaders(accessToken, requestBody, endpointType, clientHeaders = {}) {
    const headers = {
      'content-type': 'application/json',
      authorization: `Bearer ${accessToken}`,
      'user-agent': this.userAgent,
      'x-factory-client': 'cli',
      connection: 'keep-alive'
    }

    // Anthropic ç‰¹å®šå¤´
    if (endpointType === 'anthropic') {
      headers['accept'] = 'application/json'
      headers['anthropic-version'] = '2023-06-01'
      headers['x-api-key'] = 'placeholder'
      headers['x-api-provider'] = 'anthropic'

      // å¤„ç† anthropic-beta å¤´
      const reasoningLevel = this._getReasoningLevel(requestBody)
      if (reasoningLevel) {
        headers['anthropic-beta'] = 'interleaved-thinking-2025-05-14'
      }
    }

    // OpenAI ç‰¹å®šå¤´
    if (endpointType === 'openai') {
      headers['x-api-provider'] = 'azure_openai'
    }

    // ç”Ÿæˆä¼šè¯ IDï¼ˆå¦‚æœå®¢æˆ·ç«¯æ²¡æœ‰æä¾›ï¼‰
    headers['x-session-id'] = clientHeaders['x-session-id'] || this._generateUUID()

    return headers
  }

  /**
   * å¤„ç†è¯·æ±‚ä½“ï¼ˆæ³¨å…¥ system prompt ç­‰ï¼‰
   */
  _processRequestBody(requestBody, endpointType) {
    const processedBody = { ...requestBody }

    // ç¡®ä¿ stream å­—æ®µå­˜åœ¨
    if (processedBody.stream === undefined) {
      processedBody.stream = true
    }

    // Anthropic ç«¯ç‚¹ï¼šå¤„ç† thinking å­—æ®µ
    if (endpointType === 'anthropic') {
      if (this.systemPrompt) {
        const promptBlock = { type: 'text', text: this.systemPrompt }
        if (Array.isArray(processedBody.system)) {
          const hasPrompt = processedBody.system.some(
            (item) => item && item.type === 'text' && item.text === this.systemPrompt
          )
          if (!hasPrompt) {
            processedBody.system = [promptBlock, ...processedBody.system]
          }
        } else {
          processedBody.system = [promptBlock]
        }
      }

      const reasoningLevel = this._getReasoningLevel(requestBody)
      if (reasoningLevel) {
        const budgetTokens = {
          low: 4096,
          medium: 12288,
          high: 24576
        }
        processedBody.thinking = {
          type: 'enabled',
          budget_tokens: budgetTokens[reasoningLevel]
        }
      } else {
        delete processedBody.thinking
      }
    }

    // OpenAI ç«¯ç‚¹ï¼šå¤„ç† reasoning å­—æ®µ
    if (endpointType === 'openai') {
      if (this.systemPrompt) {
        if (processedBody.instructions) {
          if (!processedBody.instructions.startsWith(this.systemPrompt)) {
            processedBody.instructions = `${this.systemPrompt}${processedBody.instructions}`
          }
        } else {
          processedBody.instructions = this.systemPrompt
        }
      }

      const reasoningLevel = this._getReasoningLevel(requestBody)
      if (reasoningLevel) {
        processedBody.reasoning = {
          effort: reasoningLevel,
          summary: 'auto'
        }
      } else {
        delete processedBody.reasoning
      }
    }

    return processedBody
  }

  /**
   * è·å–æ¨ç†çº§åˆ«ï¼ˆå¦‚æœåœ¨ requestBody ä¸­é…ç½®ï¼‰
   */
  _getReasoningLevel(requestBody) {
    if (!requestBody || !requestBody.model) {
      return null
    }

    const configured = this.modelReasoningMap.get(requestBody.model)
    if (!configured) {
      return null
    }

    if (!VALID_REASONING_LEVELS.has(configured)) {
      return null
    }

    return configured
  }

  /**
   * å¤„ç†éæµå¼å“åº”
   */
  async _handleNonStreamResponse(
    response,
    account,
    apiKeyData,
    requestBody,
    clientRequest,
    endpointType
  ) {
    const { data } = response

    // ä»å“åº”ä¸­æå– usage æ•°æ®
    const usage = data.usage || {}

    const model = requestBody.model || 'unknown'

    const normalizedUsage = this._normalizeUsageSnapshot(usage)
    await this._recordUsage(apiKeyData, account, model, normalizedUsage)

    const totalTokens = this._getTotalTokens(normalizedUsage)

    const usageSummary = {
      inputTokens: normalizedUsage.input_tokens || 0,
      outputTokens: normalizedUsage.output_tokens || 0,
      cacheCreateTokens: normalizedUsage.cache_creation_input_tokens || 0,
      cacheReadTokens: normalizedUsage.cache_read_input_tokens || 0
    }

    await this._applyRateLimitTracking(
      clientRequest?.rateLimitInfo,
      usageSummary,
      model,
      endpointType === 'anthropic' ? ' [anthropic]' : ' [openai]'
    )

    logger.success(`âœ… Droid request completed - Account: ${account.name}, Tokens: ${totalTokens}`)

    return {
      statusCode: 200,
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(data)
    }
  }

  /**
   * è®°å½•ä½¿ç”¨ç»Ÿè®¡
   */
  async _recordUsage(apiKeyData, account, model, usageObject = {}) {
    const totalTokens = this._getTotalTokens(usageObject)

    if (totalTokens <= 0) {
      logger.debug('ğŸª™ Droid usage æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡è®°å½•')
      return
    }

    try {
      const keyId = apiKeyData?.id
      const accountId = this._extractAccountId(account)

      if (keyId) {
        await apiKeyService.recordUsageWithDetails(keyId, usageObject, model, accountId, 'droid')
      } else if (accountId) {
        await redis.incrementAccountUsage(
          accountId,
          totalTokens,
          usageObject.input_tokens || 0,
          usageObject.output_tokens || 0,
          usageObject.cache_creation_input_tokens || 0,
          usageObject.cache_read_input_tokens || 0,
          model,
          false
        )
      } else {
        logger.warn('âš ï¸ æ— æ³•è®°å½• Droid usageï¼šç¼ºå°‘ API Key å’Œè´¦æˆ·æ ‡è¯†')
        return
      }

      logger.debug(
        `ğŸ“Š Droid usage recorded - Key: ${keyId || 'unknown'}, Account: ${accountId || 'unknown'}, Model: ${model}, Input: ${usageObject.input_tokens || 0}, Output: ${usageObject.output_tokens || 0}, Cache Create: ${usageObject.cache_creation_input_tokens || 0}, Cache Read: ${usageObject.cache_read_input_tokens || 0}, Total: ${totalTokens}`
      )
    } catch (error) {
      logger.error('âŒ Failed to record Droid usage:', error)
    }
  }

  /**
   * ç”Ÿæˆ UUID
   */
  _generateUUID() {
    return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, (c) => {
      const r = (Math.random() * 16) | 0
      const v = c === 'x' ? r : (r & 0x3) | 0x8
      return v.toString(16)
    })
  }
}

// å¯¼å‡ºå•ä¾‹
module.exports = new DroidRelayService()
